How Unladen Swallow Uses LLVM
=============================

This document tries to provide a high-level overview of how LLVM is used inside
Unladen Swallow, including details of all the optimizations implemented for
LLVM-generated Python machine code. This document should be as
developer-centric as possible: it should be able to answer questions like,
"how does Unladen Swallow determine function hotness" and also "where is that
implemented?".

TODO(collinwinter): move the CodeLifecycle wiki page into this file.

Important files
---------------

Here are the primary files that you'll be working on as part of Unladen Swallow:

- JIT/llvm_fbuilder.{h,cc} - The heart of the compiler. These files take care
  of converting CPython bytecode to LLVM IR.
- JIT/opcodes/* - These files contain the code to implement an opcode
  in LLVM IR.
- JIT/llvm_inline_functions.c - Special-cased C functions to be inlined.
- Lib/test/test_llvm.py - Tests for the JIT compiler. All code generation and
  optimization changes need tests. Tests should include both positive and
  negative cases.
- Python/eval.cc - The interpreter loop. If you need to collect new types of
  runtime feedback for an optimization, you'll be modifying this file.


Invariants
----------

- If f->f_use_jit is true, co->co_use_jit is true; if co->co_use_jit is true,
  f->f_use_jit may be true. Individual execution frames may disable LLVM for
  a number of reasons: if tracing is enabled, or if some assumptions in the
  machine code are known not to hold in this particular frame of execution.


Feedback-directed optimization
------------------------------

- TODO: Explain data gathering (r778)
- TODO: Bailing back to the interpreter

If run with `-Xjit=never` or `-Xjit=always`, Unladen Swallow will not gather any
data at runtime.

Whenever we encode assumptions about the world into the generated machine code,
we need a way to detect that those assumptions are no longer valid and recover
gracefully. These assumptions are protected by cheap-to-execute tests called
*guards*. These guards come in two flavors: fatal guards and non-fatal guards.

Fatal guards:
Let's say that we've implemented an optimization that embeds pointers to builtin
functions in the generated machine code as immediates. If someone rebinds the
`len` builtin, any pointer we've embedded to the `builtin_len` C function is no
longer valid (we should be calling the new `len` instead); since the pointers
are immediates in the machine code, the whole machine code function is invalid,
and needs to be recompiled. Because we cannot reuse the machine code once the
guard `actual_len == expected_len` fails, we say that the guard failure is
fatal.

Non-fatal guards:
By constrast, there are some guards that do not invalidate the machine code
when they fail. One such example is that machine code functions do not support
tracing: if they detect that tracing has been turned on, they immediately
bail to the interpreter. Once tracing is disabled, though, it's perfectly safe
to start using the machine code again.

Instrumentation:
- If configured with --with-instrumentation, the system will keep track of how
  many feedback maps were created. This is useful for tracking memory usage.

Relevant Files:
- Python/eval.cc - where data is actually gathered.
- JIT/RuntimeFeedback.{h,cc} - structures for recording data.
- Unittests/RuntimeFeedbackTest.cc - tests for data gathering infrastructure.


Hotness model: finding critical functions
-----------------------------------------

We use an online model to estimate which functions are most critical to an
application's performance. This model is as follows:

- Each code object has a hotness level (the co_hotness field).
    - For each function entry, add 10 to the hotness level.
    - For each loop backedge, add 1 to the hotness level.
- If the hotness level exceeds a given threshold (see eval.cc),
  compile the code object to machine code via LLVM. This check is done on
  function-entry and generator re-entry.

There several classes of functions we're trying to catch with this model:

- Straight-line utility functions (lots of invocations, low running time).
- Loop-heavy main functions (few invocations, high running time).
- Long-running generators (few invocations, long lifetime).

Miscellaneous notes:
- JIT compilation is always disabled during startup by temporarily forcing `-j
  never`. This improves startup time by disabling compilation and feedback
  collection.

Previous models:
- Simple call count-based model (10000 calls == hot). This was implemented as
  an obviously-deficient baseline to be improved upon.
- Previously, we didn't check code hotness on generator re-entry, which we
  changed to catch long-running generators that are called once.

Instrumentation:
- Passing --with-instrumentation to ./configure will cause Python to print,
  among other things, a table of function hotness at interpreter-shutdown.
  Misc/diff_hotness.py can then be used to highlight the differences between
  two of these tables, showing the effects of changes to the model.

Relevant Files:
- Python/eval.cc - definition, use of the hotness model.

Infrastructure: JIT/opcodes/*
-----------------------------

All opcode implementations are now grouped together by their effect in the
JIT/opcodes directory. One class corresponds to every opcode group. It should
expose a method for each opcode it implements. These methods should be named
after the opcode name (eg. OP_NAME) and should apply the standard range of
optimizations for the opcode.

If the opcode has optimizations, each class should also expose a safe variant,
usually denoted by OP_NAME_safe.
It may also expose optimized variants of the opcodes, using the same nameing
scheme. If the optimization can not be applied, the method must return false
and may not generate any LLVM IR.

Relevant Files:
- JIT/opcodes/*

Infrastructure: Watching dictionaries for changes
-------------------------------------------------

Dictionaries are used frequently to implement important parts of Python, such
as namespaces. In many cases, these dictionaries rarely -- or never -- change
after being initialized. There are a number of optimizations we can make if we
assume that these dictionaries never change, or if they do, that we notice and
undo whatever assumptions have been made.

Requirements:
- Need to track why each dict is being watched.
- Don't take references to watchers or dicts; destructors will clear things out.
- Set semantics.

Data structures:
- Dicts keep PySmallSet of PyCodeObject*s.
- Code objects keep PyWatchingStruct*.

Register code object to watch dict: (_PyCode_WatchDict)
    - Add dict to code object
    - Add code object to dict (_PyDict_AddWatcher)

Dict changes (_PyDict_NotifyAllWatchers):
    - For each code object in the dict watch list,
        - Set co_use_jit to 0
        - For each dict in the code object's watch list
              (_PyCode_IgnoreWatchedDicts),
            - Remove the code object from that dict's watch list
                  (_PyDict_DropWatcher)
    - Assert dict's set is empty

Code object deletion/unwatch (_PyCode_IgnoreWatchedDicts):
    - Remove code object from all watched dicts (_PyDict_DropWatcher)
    - Don't bother removing the watching struct.


Memory use: Destroying unused LLVM globals
------------------------------------------

When a user compiles a PyCodeObject to an llvm::Function, that Function holds
references to lots of other llvm::GlobalValues produced by the PyConstantMirror
, each of which holds a reference to the PyObject it's mirroring.  If the
PyCodeObject is destroyed, we want to free all the now-unused GlobalVariables
and decref their PyObjects.  We do this by running LLVM's globaldce pass
periodically.  Just before compiling a new PyCodeObject to IR in
JIT/llvm_compile.cc:_PyCode_ToLlvmIr(), we call
PyGlobalLlvmData::MaybeCollectUnusedGlobals().  If the number of GlobalValues is
at least 25% more than after the last collection, we run globaldce.  This
parallels the backoff used for the ordinary cycle detector to avoid taking
quadratic time for runs with lots of long-lived objects.

Relevant Files:
- JIT/ConstantMirror.{h,cc} - utilities for mirroring Python objects into
  constant LLVM IR types.


Optimization: LOAD_GLOBAL compile-time caching
----------------------------------------------

In the eval loop, the LOAD_GLOBAL opcode requires two PyDict_GetItem() calls to
look up builtin functions (only one PyDict_GetItem() call for objects in the
module's global namespace). Since builtin functions rarely change, we would
like to avoid the work of repeatedly looking up the builtins.

To accomplish this, we shift the PyDict_GetItem() calls from code execution-time
to code compilation-time. When compiling a LOAD_GLOBAL opcode to LLVM IR, we
try the global lookup, then the builtins lookup, and if successful, we cache
the pointer as an immediate in the generated IR. Note that this kind of caching
means that if the globals/builtins change, the machine code is no longer valid.

Python edge cases:
- The following code is perfectly legal:

    assert "foo" not in globals()
    def bar():
        foo()
        return len([])
    bar()

  This should use an unoptimized LOAD_GLOBAL implementation for looking up
  `foo()` and an optimized implementation for `len()`.
- There are `PyEval_GetGlobals()` and `PyEval_GetBuiltins()` functions; these
  are seductive, but wrong. Get the globals/builtins off the PyFrameObject
  passed in to `PyEval_EvalFrame()`.
- A given code object can be run against different globals/builtins dicts.
  Accordingly, we must keep track of which globals/builtins a code object is
  assuming and guard on those values.

Implementation:
- When a function has been selected for compilation to LLVM IR, it will ask
  the given globals/builtins dicts (as pulled off the frame object) to notify
  the code object when the dicts change. See the above Infrastructure section
  on "Watching dictionaries for changes" for how this system works.
- The optimized machine code will guard the cached pointer by testing
  co_use_jit; if this is 0, tailcall to the interpreter to continue execution.
  Otherwise (if it is 1), continue execution of the machine code, using the
  cached pointer in place of the two `PyDict_GetItem()` calls. Dicts will set
  the code object's co_use_jit field to 0 when they are modified.

Instrumentation:
- The --with-instrumentation build will tell you which functions have their
  machine code disabled due to changing globals/builtins. It can also tell you
  how many machine code functions were disabled per globals/builtins change.
- sys.setbailerror(True) will cause an exception to be raised if a function
  fails a guard (fatal or non-fatal) and bails back to the interpreter.

Relevant Files:
- Python/eval.cc: record feedback; link code objects with dicts.
- Objects/codeobject.c: functions to have code objects watch dicts.
- Objects/dictobject.c: support for having dicts push notifications to code.
- JIT/llvm_fbuilder.{h,cc}: LOAD_GLOBAL optimization code.


Optimization: direct calls to C functions
-----------------------------------------

In the interpreter loop, CALL_FUNCTION opcodes that call zero- or
single-argument C functions incur some overhead from checking that the number
of arguments matches the number of expected parameters. It is unnecessary to
incur this overhead repeatedly at runtime, since callsites with only positional
arguments cannot change the nature or number of their arguments once written,
and a C function cannot change its number of parameters.

We can take advantage of this to move argument/parameter count checking from
execution-time to compile-time. This allows us to emit direct calls to C
functions, rather than going through the more generic Python call machinery.

Implementation:
- The runtime feedback-gathering code in JIT/RuntimeFeedback.* will grow
  support for recording the underlying C function pointers from
  PyCFunctionObjects. The CALL_FUNCTION implementation in the interpreter loop
  will use this to gather information about individual callsites.
- When compiling CALL_FUNCTION opcodes, JIT/llvm_fbuilder.cc will consult the
  gathered runtime feedback. If the feedback meets its requirements (low arity,
  all C functions, number of callsite arguments matches the number of function
  parameters, etc), then fbuilder.cc will create an LLVM global value
  representing the function pointer(s) and then emit a guarded direct call to
  that function.
- The optimized function call is guarded on the type of the callable object and
  the callable object's underlying C function pointer. If either of these tests
  fail, the function bails back to the interpreter. It is intended that these
  guards will eventually be constant-propagated away.

Example code:

    def foo(l):
        l.append(5)

The actual call of the CALL_FUNCTION opcode goes from

%CALL_FUNCTION_result = call %struct._object* @_PyEval_CallFunction(
    %struct._object** %58, i32 1, i32 0) ;

to

%68 = call %struct._object* @append(%struct._object* %CALL_FUNCTION_actual_self,
    %struct._object* bitcast (%struct.PyIntObject* @14 to %struct._object*)) ;

This intentionally omits the necessary guards.

Instrumentation:
- The --with-tsc build already included instrumentation for measuring call
  overhead. This optimization supports those TSC-based hooks.
- The --with-instrumentation build includes support for measuring callsite
  arity across an application, as well as tracking the reasons why various
  callsites were forced to use the safe version of CALL_FUNCTION.


Optimization: omit untaken branches
-----------------------------------

Unladen Swallow's runtime feedback system records whether a branch was taken or
not. When compiling to LLVM IR, if we see a branch that is either 100% taken or
not taken, we will replace the other direction with a bail-to-interpreter block.

To limit the effect of mispredictions, we only optimize branches where we have
200 or more data points. This gives us greater confidence that a given branch
has stabilized.

In order to keep this optimization minimally invasive, we actually compile
code in the not-taken direction to LLVM IR, but the conditional branch doesn't
jump to it. We rely on LLVM's dead-code elimination passes to remove any
truly untaken branches (the code may have additional in-edges).

Instrumentation:
- The --with-instrumentation build collects statistics on how many conditional
  branches were compiled to IR, how many we were able to optimize, how many
  failed to optimize due to inconsistency, and how many failed to optimize due
  to insufficient data.


Optimization: specialized binary operators
------------------------------------------

We take advantage of the gathered type feedback to optimize certain binary
operators based on their input types. For example, if we know that a certain
+ operator always receives integer arguments, we can omit a lot of unnecessary
indirection.

Example codegen:

def add(a, b):
    return a + b

def main():
    for x in xrange(15000):
        add(x, x + 1)


Before type-based specialization:

%binop_result = call %struct._object* @PyNumber_Add(%struct._object* %47,
                                                    %struct._object* %local_b2)

After type-based specialization:

  %44 = load %struct._typeobject** %43
  %45 = icmp eq %struct._typeobject* %44, @PyInt_Type
  br i1 %45, label %46, label %BINOP_OPT_bail

; <label>:46                                      ; preds = %line_start
  %47 = getelementptr inbounds %struct._object* %local_b2, i32 0, i32 1
  %48 = load %struct._typeobject** %47
  %49 = icmp eq %struct._typeobject* %48, @PyInt_Type
  br i1 %49, label %50, label %BINOP_OPT_bail

; <label>:50                                      ; preds = %46
  %51 = getelementptr inbounds %struct._object* %42, i32 1, i32 0
  %52 = load i32* %51
  %53 = getelementptr inbounds %struct._object* %local_b2, i32 1, i32 0
  %54 = load i32* %53
  %55 = add nsw i32 %54, %52
  %56 = xor i32 %55, %52
  %57 = icmp slt i32 %56, 0
  br i1 %57, label %58, label %_PyLlvm_BinAdd_Int.exit

_PyLlvm_BinAdd_Int.exit:                          ; preds = %50, %58
  %61 = call %struct._object* @PyInt_FromLong(i32 %55) nounwind
  %62 = icmp eq %struct._object* %61, null
  br i1 %62, label %BINOP_OPT_bail, label %BINOP_OPT_success


While this certainly increases code size, it eliminates the following
indirection/runtime overhead:

- Call PyNumber_Add().
- ...which calls binary_op1().
- ...which fetches the foo->ob_type->tp_as_number->nb_add function pointer.
- ...and then does a bunch of checks to make sure that both operands are the
     same types.
- ...and then calls foo->ob_type->tp_as_number->nb_add.
- ...which checks that both arguments are of PyInt_Type.
- ...and then converts them to machine longs.
- ...and finally does the work to add them and return up the call stack.


Supported operators:
- List and tuple subscription with an integer index.
- Int/int addition, subtraction, multiplication, division and modulus.
- Float/float addition, subtraction, multiplication, division and modulus.
- Float/int multiplication and division.
- String and unicode formatting.

Instrumentation:
- The --with-instrumentation build includes measurements for binary operator
  inlining.

Relevant revisions:
- http://code.google.com/p/unladen-swallow/source/detail?r=957


Optimization: specialized comparison operators
----------------------------------------------

We take advantage of the gathered type feedback to optimize certain comparison
operators based on their input types. For example, if we know that a certain ==
operator always receives integer arguments, we can omit a lot of unnecessary
indirection.

While this certainly increases code size, it eliminates quite a bit of
indirection.  This optimization works in almost the exact same way as the
previous binary operator specializing patch.

Supported operators:
- <, <=, ==, !=, >, >= for integers
- > for floats

Instrumentation:
- The --with-instrumentation build includes measurements for comparison
  operator inlining.


Optimization: IMPORT_NAME compile-time caching
----------------------------------------------

We optimize import statements in much the same way as LOAD_GLOBAL: rather than
doing the lookup at runtime, we do the lookup at compile-time and cache a
guarded pointer in the generated machine code.

Import statements are worth optimizing because the implementation of Django
templates has very hot code that looks like this:

  def escape(value):
    from django.utils.safestring import mark_for_escaping
    return mark_for_escaping(value)

That import will hit the sys.modules dict, so it is relatively fast, but it is
not free: there is a lot of work done before hitting sys.modules, such as
parsing the module name to be imported. That work turns out to be pretty
expensive, worth roughly 20% of Django template performance.

The interpreter's runtime feedback mechanism has been extended to track which
modules are returned from import statements. If a given import is monomorphic
(always returns the same module), we cache a pointer to the module in the
generated IR. The cached pointer is guarded by watching sys.modules and
__builtins__ for changes.


Guards:
- If the __import__ builtin is overridden, it can return arbitrary objects.
  Accordingly, we guard against changes to the builtins dict. Shadowing
  __import__ in globals() does not change imports, so we do not guard on it.
- If sys.modules changes, the machine code will be invalidated.


Instrumentation:
- The --with-instrumentation build includes measurements for how many import
  statement were compiled; how many import statements were successfully
  optimized; etc.


Optimization: specialization of builtin functions
-------------------------------------------------

Some builtin functions, like len(), go through a huge amount of indirection
before doing some trivial amount of work. We specialize these per type, thus
avoiding the indirection and inlining the trivial amount of work at the end of
the chain.

Supported builtins:
- len() of str, unicode, list, tuple and dict.


Instrumentation:
- The --with-instrumentation build includes data on builtin specialization.

Future work:
We currently hand-specialize the builtins for a limited number of types. This
should be replaced by a system where we use Clang to compile builtins and other
runtime functions to LLVM IR, then use a combination of top-down inlining (ala
Waddell & Dybvig, 2004) and alias analysis to perform the specialization
automatically.

The manual specialization solution is suboptimal and will not scale. However, it
sets a baseline performance level that the future Clang-based builtins
specialization system will be required to meet or ideally exceed.

Relevant files:
- JIT/llvm_inline_functions.c: definition of the manually-specialized
  functions.
- JIT/opcodes/call.{cc,h}: special-casing for this kind of inlining during
  bytecode-to-LLVM IR compilation.


Optimization: LLVM Metadata based type annotations
--------------------------------------------------

With Type-based Alias Analysis it's possible to generate extensive optimization
opportunities. To access these, we implement three passes, which handle LLVM
Metadata information.

PyTypeMarkingPass:
    This pass adds Metadata to certain function calls, which generate PyObjects
    of a known subtype. This often occurs in JIT/llvm_inline_functions.c,
    where function calls like "PyInt_FromLong" are exposed to the JIT.
    While the inliner does copy metadata, it would be more work to add Metadata
    to the code loaded from a BC file.
    Example:
        PyInt_FromLong -> PyIntObject
        PyFloat_FromDouble -> PyFloatObject

PyTBAliasAnalysis:
    Adds Type-based Alias Analysis to the Unladen Swallow optimization suite.
    This analysis helps differentiating between Python stack traffic and
    IncRef/DecRef.

PyTypeGuardRemovalPass:
    Removes type guards, which can be proven always true because
    of type annotations. This currently works for intermediate values in
    expressions optimized by binary operator inlining.
    This does not work over stack push/pop so these have to be removed first
    by other optimizations (GVN and SCCP).

Future work:
    - Add type annotations based on successfully passed type guards.
    - Use precalculated stack positions to free the Alias Analysis from
      removing stack traffic. This would allow us to run the guard removal
      pass earlier during the optimizations.

Relevant files:
- JIT/llvm_fbuilder.cc JIT/opcodes/*: Add Metadata to certain instructions.
- JIT/PyTBAliasAnalysis.{h,cc}: Various passes to deal with type annotations.
